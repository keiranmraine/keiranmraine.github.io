{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Husband, father, software-developer / bioinformatician. Current position Principal Software developer, Cancer Ageing and Somatic Mutation (CASM), Wellcome Sanger Institute . Much of my development time is spent improving the usability of the teams code, specifically with the intent of making it possible for collaborators to reproduce findings independently using the same software stack. You can find more detail here . Global Projects I am a member of the following initiatives: ICGC ARGO Data Management Working group Occasional consultation, not regular attendee PPCG Technical working group Occasional consultation, not regular attendee I was heavily involved in the ICGC PanCancer Analysis of Whole Genomes as a member of: Technical working group QC working group GitHub ORCID You can view my publications via the ORCID system: https://orcid.org/0000-0002-5634-1539","title":"Home"},{"location":"#home","text":"Husband, father, software-developer / bioinformatician.","title":"Home"},{"location":"#current-position","text":"Principal Software developer, Cancer Ageing and Somatic Mutation (CASM), Wellcome Sanger Institute . Much of my development time is spent improving the usability of the teams code, specifically with the intent of making it possible for collaborators to reproduce findings independently using the same software stack. You can find more detail here .","title":"Current position"},{"location":"#global-projects","text":"I am a member of the following initiatives: ICGC ARGO Data Management Working group Occasional consultation, not regular attendee PPCG Technical working group Occasional consultation, not regular attendee I was heavily involved in the ICGC PanCancer Analysis of Whole Genomes as a member of: Technical working group QC working group","title":"Global Projects"},{"location":"#github","text":"","title":"GitHub"},{"location":"#orcid","text":"You can view my publications via the ORCID system: https://orcid.org/0000-0002-5634-1539","title":"ORCID"},{"location":"Education_and_Skills/","text":"Education & Skills Education M.Sc. Bioinformatics University of Manchester 2001-2002 Project Finding the immunogenic needle in the immunological haystack: a comparative study of context dependent text mining. Edward Jenner Institute for Vaccine Research Supervisor Dr Darren Flower B.Sc. (Hons) Biomedical Sciences, 2:1 University of Durham 1998-2001 Project The Chromosome 19 Limb-Girdle Muscular Dystrophy Gene (LGMD2I). Candidate Gene Analysis. University of Durham, Department of Biosciences Supervisor Dr Rumaisa Bashir Skills Soft skills Active Bystander - September 2020 Wellcome Sanger Institute Management in Action - 2018-2019 Talking Ape Unconscious Bias in Recruitment and Selection - October 2017 Challenge Consultancy AGILE: Agile Testing Strategies - July 2013 Learning Tree - course 1815 Scripting and programming languages python (3) General scripting, packages Database access Web-services Cython, pyCuda Recent skill development perl 18 years experience Object Oriented design CGI, DBI bash proficient Cloud, virtualisation and related tools AWS General use of virtual instances and volumes. Polly for training video voice-overs. OpenStack Tools for host management and deployment, applicable to other environments but used mainly here: Packer - image building Terraform - infrastructure deployment Containerisation tools Docker Use of build stages for smaller deployments Experience of docker-swarm Singularity Mainly usage and ensuring docker containers are compatible. More portable to develop via Dockerfile and convert. Specialised registries - workflow + container Dockstore Web-development nginx HTML, CSS - intermediate JavaScript - proficient React - slightly stale Databases Oracle - main experience including query optimisation. SQLite, PostgreSQL, MySQL Version control git/GitHub/GitLab GitFlow/HubFLow methodology preferred SVN CVS Genome browsers JBrowse Configuration and usage Plugin development GBrowse Configuration and usage IGV Configuration and usage Other development skills This section details languages I have some experience but would not consider myself to be proficient. Workflow languages Common Workflow Language Nexflow - training and debugging of flows generated by others only NodeJS Microservice development C/C++ Compilation, Makefile correction Online courses, Debugging R Debugging, investigating and cleaning up code for pipelines/external use. Library installation Message Queues RabbitMQ (JAVA) JAVA Used intermittently over 15 years, stale Ellexus optimization tools I/O profiling - See collaboration whitepaper and Letter","title":"Education & Skills"},{"location":"Education_and_Skills/#education-skills","text":"","title":"Education &amp; Skills"},{"location":"Education_and_Skills/#education","text":"","title":"Education"},{"location":"Education_and_Skills/#msc-bioinformatics","text":"University of Manchester 2001-2002 Project Finding the immunogenic needle in the immunological haystack: a comparative study of context dependent text mining. Edward Jenner Institute for Vaccine Research Supervisor Dr Darren Flower","title":"M.Sc. Bioinformatics"},{"location":"Education_and_Skills/#bsc-hons-biomedical-sciences-21","text":"University of Durham 1998-2001 Project The Chromosome 19 Limb-Girdle Muscular Dystrophy Gene (LGMD2I). Candidate Gene Analysis. University of Durham, Department of Biosciences Supervisor Dr Rumaisa Bashir","title":"B.Sc. (Hons) Biomedical Sciences, 2:1"},{"location":"Education_and_Skills/#skills","text":"","title":"Skills"},{"location":"Education_and_Skills/#soft-skills","text":"Active Bystander - September 2020 Wellcome Sanger Institute Management in Action - 2018-2019 Talking Ape Unconscious Bias in Recruitment and Selection - October 2017 Challenge Consultancy AGILE: Agile Testing Strategies - July 2013 Learning Tree - course 1815","title":"Soft skills"},{"location":"Education_and_Skills/#scripting-and-programming-languages","text":"python (3) General scripting, packages Database access Web-services Cython, pyCuda Recent skill development perl 18 years experience Object Oriented design CGI, DBI bash proficient","title":"Scripting and programming languages"},{"location":"Education_and_Skills/#cloud-virtualisation-and-related-tools","text":"AWS General use of virtual instances and volumes. Polly for training video voice-overs. OpenStack Tools for host management and deployment, applicable to other environments but used mainly here: Packer - image building Terraform - infrastructure deployment Containerisation tools Docker Use of build stages for smaller deployments Experience of docker-swarm Singularity Mainly usage and ensuring docker containers are compatible. More portable to develop via Dockerfile and convert. Specialised registries - workflow + container Dockstore","title":"Cloud, virtualisation and related tools"},{"location":"Education_and_Skills/#web-development","text":"nginx HTML, CSS - intermediate JavaScript - proficient React - slightly stale","title":"Web-development"},{"location":"Education_and_Skills/#databases","text":"Oracle - main experience including query optimisation. SQLite, PostgreSQL, MySQL","title":"Databases"},{"location":"Education_and_Skills/#version-control","text":"git/GitHub/GitLab GitFlow/HubFLow methodology preferred SVN CVS","title":"Version control"},{"location":"Education_and_Skills/#genome-browsers","text":"JBrowse Configuration and usage Plugin development GBrowse Configuration and usage IGV Configuration and usage","title":"Genome browsers"},{"location":"Education_and_Skills/#other-development-skills","text":"This section details languages I have some experience but would not consider myself to be proficient. Workflow languages Common Workflow Language Nexflow - training and debugging of flows generated by others only NodeJS Microservice development C/C++ Compilation, Makefile correction Online courses, Debugging R Debugging, investigating and cleaning up code for pipelines/external use. Library installation Message Queues RabbitMQ (JAVA) JAVA Used intermittently over 15 years, stale Ellexus optimization tools I/O profiling - See collaboration whitepaper and Letter","title":"Other development skills"},{"location":"conferences/","text":"Conferences and meetings 2018 December: AWS partner event: HPC in Healthcare and Life Sciences Also see whitepaper and Letter June: GCC-BOSC 2018 Modernising somatic mutation calling pipelines with Open Source tools and Containers Poster Contribution to CoFest: ensembl-to-jbrowse 2017 July: ISMB/ECCB 2017 CGP as a Service (CGPaaS) - From data submission to results using your web-browser Poster 2014 January: GMOD community meeting Challenges in providing a genome browser for large scale projects Poster Updates from the Cancer Genome Project Lightning talk - Slides","title":"Conferences and meetings"},{"location":"conferences/#conferences-and-meetings","text":"","title":"Conferences and meetings"},{"location":"conferences/#2018","text":"December: AWS partner event: HPC in Healthcare and Life Sciences Also see whitepaper and Letter June: GCC-BOSC 2018 Modernising somatic mutation calling pipelines with Open Source tools and Containers Poster Contribution to CoFest: ensembl-to-jbrowse","title":"2018"},{"location":"conferences/#2017","text":"July: ISMB/ECCB 2017 CGP as a Service (CGPaaS) - From data submission to results using your web-browser Poster","title":"2017"},{"location":"conferences/#2014","text":"January: GMOD community meeting Challenges in providing a genome browser for large scale projects Poster Updates from the Cancer Genome Project Lightning talk - Slides","title":"2014"},{"location":"profile/","text":"Work history 2002-2007 I originally joined the Cancer Genome Project (CGP) at the Sanger Institute (now CASM) in 2002 on completion of my M.Sc in Bioinformatics. At that time I was responsible for developing an integrated Laboratory Information Management System (LIMS) to support PCR heteroduplex analysis. This involved database design, Perl-CGI web-development and integration of TECAN lab robots. This system continues to be used largely unchanged for sample tracking in the CASM laboratory. During this time I also contributed to very early versions of the COSMIC database . During 2004 CGP moved heavily into capillary sequencing (also known as Sanger sequencing ). Once the LIMS was adapted I moved on to developing a high throughput pipeline to run the autoCSA analysis software. The control software was written using JAVA, and a public desktop application, a.k.a. StandaloneCSA , was created for the external use and release early 2007. This was a large project involving many developers working together on various aspects including database design, the analysis pipeline itself and a web-application used for visual inspection of the called variants by scientific staff. 2007-2009 In 2007 I left the CGP and joined Roche Pharmaceuticals as a Clinical Programmer. Here I gained experience of good clinical practice while building data capture and cleaning tools using Oracle Clinical . 2009-Current After 2 years at Roche I returned to the CGP in 2009. Although this may seem an unusual career choice the advent of next generation sequencing had significantly changed the challenges and work being undertaken. One of the primary differences in the data was the sheer volume being generated. Analysis pipelines The initial project was to create a pipeline to run the mapping of Illumina paired-end sequencing , supporting multiple species and builds. Over 6 months I extended an existing database, developed a new web-application and file tracking system to control the scheduling and flow of data into and out of a Platform LSF compute farm. After the initial 6 months the success of the project resulted in it being extended to support the downstream analysis tools with more of the team moving in to aid in both support and development. During this phase of the project the web-interfaces were extended significantly to allow the scientific staff to manage their data and analysis. The system proved to be a work-horse for the group for 6 years. The core jobs submission mechanism has been reworked by the group in the last few years, but the core database and much of the web-application has remained largely unchanged. Genome browsers As part of the analysis pipeline work I brought GBrowse into the the group as a centrally managed genome browser. GBrowse relies on server side compute and a traditional database backend to provide the images. In 2012 JBrowse matured to a state where BAM data could be directly handled in the browser. Due to this, and no further development on GBrowse, we have been actively working with the GMOD community to ensure features of GBrowse that our scientific staff consider important are replicated. We formally retired our GBrowse instance a few years ago. During 2016 I developed a plugin for JBrowse, proportionalMultiBw (prototyped in my own time) to aid in the visualization of allele fraction in regions of very high sequencing depth. I've additionally produced a general purpose tool for automating generation of screen shots, see cgpJBrowseToolkit . The new JBrowse 2 embraces this screenshot feature making this a core component of this new version of the tool (not based on my code). Global projects In 2014 the CGP became involved in the ICGC PanCancer Analysis of Whole Genomes project. As part of this a significant portion of the IT team spent 6 months preparing all of the core algorithms we use in house for public use. In addition to this work I was a member of the technical working group advising on tools, mapping strategy and creating tools to aid the submission of the raw sequencing data from the sequencing sites around the world. Working closely with the IT group at OICR I implemented the 'Sanger' pipeline into the framework to be used in multiple data centers around the world. This pipeline ran successfully in 13 locations on a variety of base infrastructures including AWS, Azure, OpenStack and traditional HPC. Following on from this project I continue to advise in the ICGC-ARGO data management group and the [PPCG][ ppcg-url technical working group. Containers for reproducible science In 2016 I developed cgpbox as an initial prototype providing our wholegenome analysis platform in a convenient to use docker image. This was well received and we continue to build on this with the dockstore framework (see above). Variations on \"cgpbox\" have been used to drive the PPCG project analysis and are in use by ICGC-ARGO at GDC Following on from this I have lead the initiative to convert all of our analysis pipelines to use containers. This has allowed us to offer our users the ability to fix algorithm versions for the life of their analysis projects. Full stack development experience During 2017 I prototyped a full-stack system for receipt of external sequencing data via web-interface and S3, backed by web-services hosted in our OpenStack flexible compute farm. Some of this can be viewed here . Following on from this a small team was formed to take this project further, developing the service architecture to validate the input data. Sadly the decision was taken to pause this project, however a great deal of new technology was investigated and this has proven to be valuable for subsequent projects. Readers note Above I have detailed projects that I have been heavily involved in. Although I may have worked exclusively on some of these it should be noted that I value the advice and expertise of those around me. The 'cancerit' team is an excellent group of people from varied backgrounds who draw on each others strengths. The team is also supported by the core informatics and infrastructure groups at the Sanger Institute.","title":"Work history"},{"location":"profile/#work-history","text":"","title":"Work history"},{"location":"profile/#2002-2007","text":"I originally joined the Cancer Genome Project (CGP) at the Sanger Institute (now CASM) in 2002 on completion of my M.Sc in Bioinformatics. At that time I was responsible for developing an integrated Laboratory Information Management System (LIMS) to support PCR heteroduplex analysis. This involved database design, Perl-CGI web-development and integration of TECAN lab robots. This system continues to be used largely unchanged for sample tracking in the CASM laboratory. During this time I also contributed to very early versions of the COSMIC database . During 2004 CGP moved heavily into capillary sequencing (also known as Sanger sequencing ). Once the LIMS was adapted I moved on to developing a high throughput pipeline to run the autoCSA analysis software. The control software was written using JAVA, and a public desktop application, a.k.a. StandaloneCSA , was created for the external use and release early 2007. This was a large project involving many developers working together on various aspects including database design, the analysis pipeline itself and a web-application used for visual inspection of the called variants by scientific staff.","title":"2002-2007"},{"location":"profile/#2007-2009","text":"In 2007 I left the CGP and joined Roche Pharmaceuticals as a Clinical Programmer. Here I gained experience of good clinical practice while building data capture and cleaning tools using Oracle Clinical .","title":"2007-2009"},{"location":"profile/#2009-current","text":"After 2 years at Roche I returned to the CGP in 2009. Although this may seem an unusual career choice the advent of next generation sequencing had significantly changed the challenges and work being undertaken. One of the primary differences in the data was the sheer volume being generated.","title":"2009-Current"},{"location":"profile/#analysis-pipelines","text":"The initial project was to create a pipeline to run the mapping of Illumina paired-end sequencing , supporting multiple species and builds. Over 6 months I extended an existing database, developed a new web-application and file tracking system to control the scheduling and flow of data into and out of a Platform LSF compute farm. After the initial 6 months the success of the project resulted in it being extended to support the downstream analysis tools with more of the team moving in to aid in both support and development. During this phase of the project the web-interfaces were extended significantly to allow the scientific staff to manage their data and analysis. The system proved to be a work-horse for the group for 6 years. The core jobs submission mechanism has been reworked by the group in the last few years, but the core database and much of the web-application has remained largely unchanged.","title":"Analysis pipelines"},{"location":"profile/#genome-browsers","text":"As part of the analysis pipeline work I brought GBrowse into the the group as a centrally managed genome browser. GBrowse relies on server side compute and a traditional database backend to provide the images. In 2012 JBrowse matured to a state where BAM data could be directly handled in the browser. Due to this, and no further development on GBrowse, we have been actively working with the GMOD community to ensure features of GBrowse that our scientific staff consider important are replicated. We formally retired our GBrowse instance a few years ago. During 2016 I developed a plugin for JBrowse, proportionalMultiBw (prototyped in my own time) to aid in the visualization of allele fraction in regions of very high sequencing depth. I've additionally produced a general purpose tool for automating generation of screen shots, see cgpJBrowseToolkit . The new JBrowse 2 embraces this screenshot feature making this a core component of this new version of the tool (not based on my code).","title":"Genome browsers"},{"location":"profile/#global-projects","text":"In 2014 the CGP became involved in the ICGC PanCancer Analysis of Whole Genomes project. As part of this a significant portion of the IT team spent 6 months preparing all of the core algorithms we use in house for public use. In addition to this work I was a member of the technical working group advising on tools, mapping strategy and creating tools to aid the submission of the raw sequencing data from the sequencing sites around the world. Working closely with the IT group at OICR I implemented the 'Sanger' pipeline into the framework to be used in multiple data centers around the world. This pipeline ran successfully in 13 locations on a variety of base infrastructures including AWS, Azure, OpenStack and traditional HPC. Following on from this project I continue to advise in the ICGC-ARGO data management group and the [PPCG][ ppcg-url technical working group.","title":"Global projects"},{"location":"profile/#containers-for-reproducible-science","text":"In 2016 I developed cgpbox as an initial prototype providing our wholegenome analysis platform in a convenient to use docker image. This was well received and we continue to build on this with the dockstore framework (see above). Variations on \"cgpbox\" have been used to drive the PPCG project analysis and are in use by ICGC-ARGO at GDC Following on from this I have lead the initiative to convert all of our analysis pipelines to use containers. This has allowed us to offer our users the ability to fix algorithm versions for the life of their analysis projects.","title":"Containers for reproducible science"},{"location":"profile/#full-stack-development-experience","text":"During 2017 I prototyped a full-stack system for receipt of external sequencing data via web-interface and S3, backed by web-services hosted in our OpenStack flexible compute farm. Some of this can be viewed here . Following on from this a small team was formed to take this project further, developing the service architecture to validate the input data. Sadly the decision was taken to pause this project, however a great deal of new technology was investigated and this has proven to be valuable for subsequent projects.","title":"Full stack development experience"},{"location":"profile/#readers-note","text":"Above I have detailed projects that I have been heavily involved in. Although I may have worked exclusively on some of these it should be noted that I value the advice and expertise of those around me. The 'cancerit' team is an excellent group of people from varied backgrounds who draw on each others strengths. The team is also supported by the core informatics and infrastructure groups at the Sanger Institute.","title":"Readers note"},{"location":"blogs/","text":"Listing 2021 Exposing singularity images as executables ssh, proxies and multiplexing Refreshing my site","title":"Listing"},{"location":"blogs/#listing","text":"","title":"Listing"},{"location":"blogs/#2021","text":"Exposing singularity images as executables ssh, proxies and multiplexing Refreshing my site","title":"2021"},{"location":"blogs/2021/05/updating-my-site/","text":"Refreshing my site I've decided to refresh this for a couple of reasons: It looks a little tired. Originally created doing it \"my way\", mainly as a \"live CV\". I feel like I have something to contribute via blog posts. Interested in including some user interaction/feedback. Choosing a site generator My original site already worked from markdown files and I want to continue like this. I pretty much write all my documentation in markdown. Here are the items I looked at: Jekyll MkDocs Material for MkDocs Jekyll Jekyll is a well established static site generator, with direct integration into github pages. This seemed like the obvious place to start as I'm going to host it on github-pages. I quickly found that if you've worked with other site generators Jekyll can feel overly complex. I suspect this is due to flexibility, but for what I needed it was too much. I also wasn't a big fan of the available themes. MkDocs Full disclosure, I've used MkDocs before for my work at Sanger and I already know how to use it. Quite simply if you want to provide a site that is easy to navigate all you need to do in MkDocs is structure things in a document tree as you want to see it in the site menu: docs/ \u251c\u2500\u2500 blogs \u2502 \u2514\u2500\u2500 2021 \u2502 \u2514\u2500\u2500 05 \u2502 \u2514\u2500\u2500 updating-my-site.md \u2514\u2500\u2500 index.md The above requires no configuration at all, but obviously you still want to do some. Before I go further, I want my site to have a clean look and feel. MkDocs has a limited number of themes, Read the Docs isn't bad but there is an extension project which I recently used and have found it good. As I'm leaning towards MkDocs over Jekyll, lets take that next step before I get bogged down on layout and content... MkDocs Material This was the selected tool (at time of writing). As the title suggests, this is MkDocs focused on Material Design components. The first thing you notice when you apply the material theme is that you automatically get a left and right sidebar. The left sidebar gives you the site navigation, very similar to the document tree: This will automatically change to a hamburger menu button if the view is narrow. On the right the table of content for the current document is shown. Interaction As I'm thinking of writing blog posts semi-regularly it makes sense to give people the ability to interact via comments. This is hosted in the GitHub eco-system and I don't really want to be pushing people to another service for comments so I did a little digging and found utterances . This basically uses the repositories issue tracking system to allow comments, see it in action below. Comments","title":"Refreshing my site"},{"location":"blogs/2021/05/updating-my-site/#refreshing-my-site","text":"I've decided to refresh this for a couple of reasons: It looks a little tired. Originally created doing it \"my way\", mainly as a \"live CV\". I feel like I have something to contribute via blog posts. Interested in including some user interaction/feedback.","title":"Refreshing my site"},{"location":"blogs/2021/05/updating-my-site/#choosing-a-site-generator","text":"My original site already worked from markdown files and I want to continue like this. I pretty much write all my documentation in markdown. Here are the items I looked at: Jekyll MkDocs Material for MkDocs","title":"Choosing a site generator"},{"location":"blogs/2021/05/updating-my-site/#jekyll","text":"Jekyll is a well established static site generator, with direct integration into github pages. This seemed like the obvious place to start as I'm going to host it on github-pages. I quickly found that if you've worked with other site generators Jekyll can feel overly complex. I suspect this is due to flexibility, but for what I needed it was too much. I also wasn't a big fan of the available themes.","title":"Jekyll"},{"location":"blogs/2021/05/updating-my-site/#mkdocs","text":"Full disclosure, I've used MkDocs before for my work at Sanger and I already know how to use it. Quite simply if you want to provide a site that is easy to navigate all you need to do in MkDocs is structure things in a document tree as you want to see it in the site menu: docs/ \u251c\u2500\u2500 blogs \u2502 \u2514\u2500\u2500 2021 \u2502 \u2514\u2500\u2500 05 \u2502 \u2514\u2500\u2500 updating-my-site.md \u2514\u2500\u2500 index.md The above requires no configuration at all, but obviously you still want to do some. Before I go further, I want my site to have a clean look and feel. MkDocs has a limited number of themes, Read the Docs isn't bad but there is an extension project which I recently used and have found it good. As I'm leaning towards MkDocs over Jekyll, lets take that next step before I get bogged down on layout and content...","title":"MkDocs"},{"location":"blogs/2021/05/updating-my-site/#mkdocs-material","text":"This was the selected tool (at time of writing). As the title suggests, this is MkDocs focused on Material Design components. The first thing you notice when you apply the material theme is that you automatically get a left and right sidebar. The left sidebar gives you the site navigation, very similar to the document tree: This will automatically change to a hamburger menu button if the view is narrow. On the right the table of content for the current document is shown.","title":"MkDocs Material"},{"location":"blogs/2021/05/updating-my-site/#interaction","text":"As I'm thinking of writing blog posts semi-regularly it makes sense to give people the ability to interact via comments. This is hosted in the GitHub eco-system and I don't really want to be pushing people to another service for comments so I did a little digging and found utterances . This basically uses the repositories issue tracking system to allow comments, see it in action below.","title":"Interaction"},{"location":"blogs/2021/05/updating-my-site/#comments","text":"","title":"Comments"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/","text":"ssh, proxies and multiplexing It can be time consuming to get your ssh connections with proxies and multiplexing working well. Here I've dumped what I used from a Mac laptop to connect to my work system. Hosts and routes have been changed. It's mainly for my reference, but may be useful for others. Options A longer example can be found at the end of this document, not all options will be detailed. As always see the man page for full explanations or other uses/defaults. ProxyJump If you need to connect to a host as more than one user, which doesn't match the ssh gateway user, you need to include the user that authenticates at the ssh-gateway in the ProxyJump directive. e.g. ssh-gateway user = bob internal-network users = bob,fred # if # Host work # ProxyJump bob@ssh.someplace.ac.uk bob@home$ ssh work ... ssh auth ... bob@work$ bob@home$ ssh fred@work # uses bob to pass through ssh-gateway ServerAliveInterval How frequently to check the server is still accessible: useful to ensure connections aren't dropped prevent connection being dropped due to inactivity ServerAliveCountMax How many ServerAliveInterval messages without a response are acceptable before abandoning the connection. ControlMaster Send data for the same user and same host down a single TCP connection (also see ControlPath). Saves authenticating each time you open another terminal for the same host+user... type less :) Full example Includes comments that may point you to useful docs: Host * XAuthLocation /opt/X11/bin/xauth Compression yes ServerAliveInterval 60 ServerAliveCountMax 2 ForwardX11Timeout 100w Ciphers aes128-ctr # share 1 ssh connection for same host/port/user ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r Host proxies HostName somehost # do ssh gateway dance and drop me onto the host I really want ProxyJump ssh.someplace.ac.uk # Point your SOCKS proxy to localhost:8999 to pass all network through your # ssh connection # - recommend setting bypass rules for zoom/webex etc DynamicForward 8999 # Forwarding - good for databases LocalForward 25000 172.XX.XX.XXX:1521 # a machine on my home network with no DNS name: Host my-nano HostName 192.168.0.90 # a host within company network, without DNS name: Host unnamed-host HostName 172.27.22.23 ProxyJump ssh.someplace.ac.uk # a host within company network Host named-host HostName somehost ProxyJump user@ssh.someplace.ac.uk Comments","title":"ssh, proxies and multiplexing"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#ssh-proxies-and-multiplexing","text":"It can be time consuming to get your ssh connections with proxies and multiplexing working well. Here I've dumped what I used from a Mac laptop to connect to my work system. Hosts and routes have been changed. It's mainly for my reference, but may be useful for others.","title":"ssh, proxies and multiplexing"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#options","text":"A longer example can be found at the end of this document, not all options will be detailed. As always see the man page for full explanations or other uses/defaults.","title":"Options"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#proxyjump","text":"If you need to connect to a host as more than one user, which doesn't match the ssh gateway user, you need to include the user that authenticates at the ssh-gateway in the ProxyJump directive. e.g. ssh-gateway user = bob internal-network users = bob,fred # if # Host work # ProxyJump bob@ssh.someplace.ac.uk bob@home$ ssh work ... ssh auth ... bob@work$ bob@home$ ssh fred@work # uses bob to pass through ssh-gateway","title":"ProxyJump"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#serveraliveinterval","text":"How frequently to check the server is still accessible: useful to ensure connections aren't dropped prevent connection being dropped due to inactivity","title":"ServerAliveInterval"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#serveralivecountmax","text":"How many ServerAliveInterval messages without a response are acceptable before abandoning the connection.","title":"ServerAliveCountMax"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#controlmaster","text":"Send data for the same user and same host down a single TCP connection (also see ControlPath). Saves authenticating each time you open another terminal for the same host+user... type less :)","title":"ControlMaster"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#full-example","text":"Includes comments that may point you to useful docs: Host * XAuthLocation /opt/X11/bin/xauth Compression yes ServerAliveInterval 60 ServerAliveCountMax 2 ForwardX11Timeout 100w Ciphers aes128-ctr # share 1 ssh connection for same host/port/user ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r Host proxies HostName somehost # do ssh gateway dance and drop me onto the host I really want ProxyJump ssh.someplace.ac.uk # Point your SOCKS proxy to localhost:8999 to pass all network through your # ssh connection # - recommend setting bypass rules for zoom/webex etc DynamicForward 8999 # Forwarding - good for databases LocalForward 25000 172.XX.XX.XXX:1521 # a machine on my home network with no DNS name: Host my-nano HostName 192.168.0.90 # a host within company network, without DNS name: Host unnamed-host HostName 172.27.22.23 ProxyJump ssh.someplace.ac.uk # a host within company network Host named-host HostName somehost ProxyJump user@ssh.someplace.ac.uk","title":"Full example"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#comments","text":"","title":"Comments"},{"location":"blogs/2021/10/containers-as-executables/","text":"Exposing singularity images as executables It can be difficult to expose some tools where they have extensive dependencies without polluting your environment. Environment modules can be used to handle this, but the interdependencies can be complex. In pipelines you need to ensure that versions of tools/algorithms are compatible and known to have been tested together so it is common to use containers to ensure the tested \"unit\" is the one that is in use. Unfortunately many environments don't allow you to use docker, but many will allow the use of singularity containers as they don't have the root escalation issues of plain docker. Get a singularity image Most tools with have a versioned docker image, singularity can pull this and convert it to a container that can be safely used without fear of root escalation: $ singularity pull docker://quay.io/wtsicgp/pcap-core:5.7.0 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... INFO: Creating SIF file... $ ls -lh *.sif -rwxrwx--- 1 kr2 ubuntu 157M Oct 18 15 :20 pcap-core_5.7.0.sif I'll leave you to decide the best place for your images, in this example images are copied into: /software/singularity-images/${program}/${program}_${version}.sif e.g. /software/singularity-images/pcap-core/pcap-core_5.7.0.sif Make the singularity image available via environment modules We need a convenient way to set up environment variables which can point to this image and simplify use for users. Modulefiles can provide this with a little lateral thinking and a few \"shim\" scripts. This blog will not cover the intricacies of Environment Modules, see the official docs . Here we setup a basic module file to make some variables available, keeping the elements you need to change at the very start of the file. File: /software/modules/modulefiles/pcap-core/5.7.0 #%Module set version 5.7.0 set program pcap-core set imagepath /software/singularity-images/$program/${program}_${version}.sif set moduleinfo \"Exposes $program tools via singularity, lustre, nfs and home accessible\" set home $env(HOME) set sing_base \"--cleanenv --home ${home}:${home} --bind ${home}:${home} ${imagepath}\" module-whatis \"$moduleinfo\" module load singularity set-alias \"${program}-shell\" \"singularity shell $sing_base\" set-alias \"${program}-exec\" \"singularity exec $sing_base\" prepend-path PATH /software/modules/installs/$program/shim-bin setenv SING_PREFIX_PCAPCORE \"singularity exec $sing_base\" The main items of note in the above script are: line description intent 2 Version of container Single place to modify 3 Name of container Single place to modify 7 Bulk of singularity options Ease of reading later commands 9 Load default singularity module assumes you have this available 10 Singularity shell alias for this container Convenience for interactive/debug use 11 Singularity exec alias for this container Convenience for interactive/debug use 12 Add \"virtual\" script location to path See \"shim scripts\", allows tab complete/discovery 13 Container specific variable See \"shim scripts\" Assuming /software/modules/modulefiles is in $MODULEPATH : $ which bwa_mem.pl # not found $ module load pcap-core/5.7.0 $ pcap-core-shell Singularity> which bwa_mem.pl /opt/wtsi-cgp/bin/bwa_mem.pl Singularity> exit $ which bwa_mem.pl # not found This is a good start, but it's not the same as just calling a script. Lets talk about shim-scripts. shim scripts A shim script is a very minimal script that can hide the complexity of calling the singularity command and behave as though the real script is available. Here we'll create a shim-script for the bwa_mem.pl script within the pcap-core container: File: /software/modules/installs/pcap-core/shim-bin/bwa_mem.pl #!/usr/bin/env bash exec $SING_PREFIX_PCAPCORE $( basename $0 ) \" $@ \" Note that the $SING_PREFIX_PCAPCORE variable from the modulefile is referenced. Now you can simply call (or tab-complete) bwa_mem.pl from the command line provided you have loaded the module: $ module load pcap-core/5.7.0 $ bwa_mem.pl -h Usage: bwa_mem.pl [ options ] [ file ( s ) ... ] ... For each command you want to expose to the user create a shim-script. Note: you shouldn't expose all the tools in the container, just the unique items the container was built for. Comments","title":"Exposing singularity images as executables"},{"location":"blogs/2021/10/containers-as-executables/#exposing-singularity-images-as-executables","text":"It can be difficult to expose some tools where they have extensive dependencies without polluting your environment. Environment modules can be used to handle this, but the interdependencies can be complex. In pipelines you need to ensure that versions of tools/algorithms are compatible and known to have been tested together so it is common to use containers to ensure the tested \"unit\" is the one that is in use. Unfortunately many environments don't allow you to use docker, but many will allow the use of singularity containers as they don't have the root escalation issues of plain docker.","title":"Exposing singularity images as executables"},{"location":"blogs/2021/10/containers-as-executables/#get-a-singularity-image","text":"Most tools with have a versioned docker image, singularity can pull this and convert it to a container that can be safely used without fear of root escalation: $ singularity pull docker://quay.io/wtsicgp/pcap-core:5.7.0 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... INFO: Creating SIF file... $ ls -lh *.sif -rwxrwx--- 1 kr2 ubuntu 157M Oct 18 15 :20 pcap-core_5.7.0.sif I'll leave you to decide the best place for your images, in this example images are copied into: /software/singularity-images/${program}/${program}_${version}.sif e.g. /software/singularity-images/pcap-core/pcap-core_5.7.0.sif","title":"Get a singularity image"},{"location":"blogs/2021/10/containers-as-executables/#make-the-singularity-image-available-via-environment-modules","text":"We need a convenient way to set up environment variables which can point to this image and simplify use for users. Modulefiles can provide this with a little lateral thinking and a few \"shim\" scripts. This blog will not cover the intricacies of Environment Modules, see the official docs . Here we setup a basic module file to make some variables available, keeping the elements you need to change at the very start of the file. File: /software/modules/modulefiles/pcap-core/5.7.0 #%Module set version 5.7.0 set program pcap-core set imagepath /software/singularity-images/$program/${program}_${version}.sif set moduleinfo \"Exposes $program tools via singularity, lustre, nfs and home accessible\" set home $env(HOME) set sing_base \"--cleanenv --home ${home}:${home} --bind ${home}:${home} ${imagepath}\" module-whatis \"$moduleinfo\" module load singularity set-alias \"${program}-shell\" \"singularity shell $sing_base\" set-alias \"${program}-exec\" \"singularity exec $sing_base\" prepend-path PATH /software/modules/installs/$program/shim-bin setenv SING_PREFIX_PCAPCORE \"singularity exec $sing_base\" The main items of note in the above script are: line description intent 2 Version of container Single place to modify 3 Name of container Single place to modify 7 Bulk of singularity options Ease of reading later commands 9 Load default singularity module assumes you have this available 10 Singularity shell alias for this container Convenience for interactive/debug use 11 Singularity exec alias for this container Convenience for interactive/debug use 12 Add \"virtual\" script location to path See \"shim scripts\", allows tab complete/discovery 13 Container specific variable See \"shim scripts\" Assuming /software/modules/modulefiles is in $MODULEPATH : $ which bwa_mem.pl # not found $ module load pcap-core/5.7.0 $ pcap-core-shell Singularity> which bwa_mem.pl /opt/wtsi-cgp/bin/bwa_mem.pl Singularity> exit $ which bwa_mem.pl # not found This is a good start, but it's not the same as just calling a script. Lets talk about shim-scripts.","title":"Make the singularity image available via environment modules"},{"location":"blogs/2021/10/containers-as-executables/#shim-scripts","text":"A shim script is a very minimal script that can hide the complexity of calling the singularity command and behave as though the real script is available. Here we'll create a shim-script for the bwa_mem.pl script within the pcap-core container: File: /software/modules/installs/pcap-core/shim-bin/bwa_mem.pl #!/usr/bin/env bash exec $SING_PREFIX_PCAPCORE $( basename $0 ) \" $@ \" Note that the $SING_PREFIX_PCAPCORE variable from the modulefile is referenced. Now you can simply call (or tab-complete) bwa_mem.pl from the command line provided you have loaded the module: $ module load pcap-core/5.7.0 $ bwa_mem.pl -h Usage: bwa_mem.pl [ options ] [ file ( s ) ... ] ... For each command you want to expose to the user create a shim-script. Note: you shouldn't expose all the tools in the container, just the unique items the container was built for.","title":"shim scripts"},{"location":"blogs/2021/10/containers-as-executables/#comments","text":"","title":"Comments"}]}
