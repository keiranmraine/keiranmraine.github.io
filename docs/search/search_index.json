{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Husband, father, software-developer / bioinformatician.</p> <p>Pronouns: he, him</p>"},{"location":"#current-position","title":"Current position","text":"<p>Head of Service, Clinical Computational Research Informatics, Health Innovation East.</p> <p>You can find more detail here.</p>"},{"location":"#recent-training","title":"Recent training","text":""},{"location":"#github","title":"GitHub","text":""},{"location":"#publications","title":"Publications","text":"<p>You can view my publications via the PubMed Bibliography service here.</p>"},{"location":"Education_and_Skills/","title":"Education &amp; Skills","text":"<ul> <li>Education \\&amp; Skills<ul> <li>Education<ul> <li>M.Sc. Bioinformatics</li> <li>B.Sc. (Hons) Biomedical Sciences, 2:1</li> </ul> </li> <li>Skills<ul> <li>Soft skills</li> <li>Scripting and programming languages</li> <li>Cloud, virtualisation and related tools</li> <li>Web-development</li> <li>Databases</li> <li>Version control</li> <li>Genome browsers</li> <li>Other development skills</li> </ul> </li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#education","title":"Education","text":""},{"location":"Education_and_Skills/#msc-bioinformatics","title":"M.Sc. Bioinformatics","text":"<ul> <li>University of Manchester</li> <li>2001-2002</li> <li>Project<ul> <li>Finding the immunogenic needle in the immunological haystack: a comparative study of context dependent text mining.</li> <li>Edward Jenner Institute for Vaccine Research</li> <li>Supervisor Dr Darren Flower</li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#bsc-hons-biomedical-sciences-21","title":"B.Sc. (Hons) Biomedical Sciences, 2:1","text":"<ul> <li>University of Durham</li> <li>1998-2001</li> <li>Project<ul> <li>The Chromosome 19 Limb-Girdle Muscular Dystrophy Gene (LGMD2I). Candidate Gene Analysis.</li> <li>University of Durham, Department of Biosciences</li> <li>Supervisor Dr Rumaisa Bashir</li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#skills","title":"Skills","text":""},{"location":"Education_and_Skills/#soft-skills","title":"Soft skills","text":"<ul> <li>Active Bystander - September 2020<ul> <li>Wellcome Sanger Institute</li> </ul> </li> <li>Management in Action - 2018-2019<ul> <li>Talking Ape</li> </ul> </li> <li>Unconscious Bias in Recruitment and Selection - October 2017<ul> <li>Challenge Consultancy</li> </ul> </li> <li>AGILE: Agile Testing Strategies - July 2013<ul> <li>Learning Tree - course 1815</li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#scripting-and-programming-languages","title":"Scripting and programming languages","text":"<ul> <li>python (3)<ul> <li>General scripting, packages</li> <li>Database access</li> <li>Web-services</li> <li>Cython, pyCuda (self taught)</li> </ul> </li> <li>perl<ul> <li>18 years experience</li> <li>Object Oriented design</li> <li>CGI, DBI</li> </ul> </li> <li>bash<ul> <li>proficient</li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#cloud-virtualisation-and-related-tools","title":"Cloud, virtualisation and related tools","text":"<ul> <li>AWS<ul> <li>General use of virtual instances and volumes.</li> <li>Polly for training video voice-overs.</li> <li>AWS Cloud Quest: CLoud Practitioner</li> </ul> </li> <li>OpenStack<ul> <li>Tools for host management and deployment, applicable to other environments but used mainly here:<ul> <li>Packer - image building</li> <li>Terraform - infrastructure deployment</li> </ul> </li> </ul> </li> <li>Containerisation tools<ul> <li>Docker<ul> <li>Use of build stages for smaller deployments</li> <li>Experience of docker-swarm</li> </ul> </li> <li>Singularity<ul> <li>Mainly usage and ensuring docker containers are compatible.</li> <li>More portable to develop via Dockerfile and convert.</li> </ul> </li> <li>Specialised registries - workflow + container<ul> <li>Dockstore</li> </ul> </li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#web-development","title":"Web-development","text":"<ul> <li>nginx</li> <li>HTML, CSS - intermediate</li> <li>JavaScript - proficient</li> <li>React - slightly stale</li> </ul>"},{"location":"Education_and_Skills/#databases","title":"Databases","text":"<ul> <li>Oracle - main experience including query optimisation.</li> <li>SQLite, PostgreSQL, MySQL</li> </ul>"},{"location":"Education_and_Skills/#version-control","title":"Version control","text":"<ul> <li>git/GitHub/GitLab<ul> <li>GitFlow/HubFLow methodology preferred</li> </ul> </li> <li>SVN</li> <li>CVS</li> </ul>"},{"location":"Education_and_Skills/#genome-browsers","title":"Genome browsers","text":"<ul> <li>JBrowse<ul> <li>Configuration and usage</li> <li>Plugin development</li> </ul> </li> <li>GBrowse<ul> <li>Configuration and usage</li> </ul> </li> <li>IGV<ul> <li>Configuration and usage</li> </ul> </li> </ul>"},{"location":"Education_and_Skills/#other-development-skills","title":"Other development skills","text":"<p>This section details languages I have some experience but would not consider myself to be proficient.</p> <ul> <li>Workflow languages<ul> <li>Common Workflow Language</li> <li>Nexflow</li> </ul> </li> <li>NodeJS<ul> <li>Microservice development</li> </ul> </li> <li>C/C++<ul> <li>Compilation, Makefile correction</li> <li>Online courses, Debugging</li> </ul> </li> <li>R<ul> <li>Debugging, investigating and cleaning up code for pipelines/external use.</li> <li>Library installation</li> </ul> </li> <li>Message Queues<ul> <li>RabbitMQ (JAVA)</li> </ul> </li> <li>JAVA<ul> <li>Used intermittently over 15 years, stale</li> </ul> </li> <li>Ellexus optimization tools<ul> <li>I/O profiling - See collaboration whitepaper and Letter</li> </ul> </li> </ul>"},{"location":"conferences/","title":"Conferences and meetings","text":"<ul> <li>Conferences and meetings<ul> <li>2018</li> <li>2017</li> <li>2014</li> </ul> </li> </ul>"},{"location":"conferences/#2018","title":"2018","text":"<ul> <li>December: AWS partner event: HPC in Healthcare and Life Sciences<ul> <li>Also see whitepaper and Letter</li> </ul> </li> <li>June: GCC-BOSC 2018<ul> <li>Modernising somatic mutation calling pipelines with Open Source tools and Containers<ul> <li>Poster</li> </ul> </li> <li>Contribution to CoFest: ensembl-to-jbrowse</li> </ul> </li> </ul>"},{"location":"conferences/#2017","title":"2017","text":"<ul> <li>July: ISMB/ECCB 2017<ul> <li>CGP as a Service (CGPaaS) - From data submission to results using your web-browser</li> <li>Poster</li> </ul> </li> </ul>"},{"location":"conferences/#2014","title":"2014","text":"<ul> <li>January: GMOD community meeting<ul> <li>Challenges in providing a genome browser for large scale projects<ul> <li>Poster</li> </ul> </li> <li>Updates from the Cancer Genome Project<ul> <li>Lightning talk - Slides</li> </ul> </li> </ul> </li> </ul>"},{"location":"profile/","title":"Work history","text":"<ul> <li>Work history<ul> <li>2002-2007</li> <li>2007-2009</li> <li>2009-2022<ul> <li>Analysis pipelines</li> <li>Genome browsers</li> <li>Global projects</li> <li>Containers for reproducible science</li> <li>Full stack development experience</li> <li>Readers note</li> </ul> </li> <li>2022 onwards</li> <li>Consultancy work<ul> <li>September 2021 - March 2022</li> </ul> </li> </ul> </li> </ul>"},{"location":"profile/#2002-2007","title":"2002-2007","text":"<ul> <li>Junior Bioinformatician</li> <li>Bioinformatician</li> <li>Seniour Bioinformatician</li> </ul> <p>I originally joined the Cancer Genome Project (CGP) at the Sanger Institute (now CASM) in 2002 on completion of my M.Sc in Bioinformatics.  At that time I was responsible for developing an integrated Laboratory Information Management System (LIMS) to support PCR heteroduplex analysis.  This involved database design, Perl-CGI web-development and integration of TECAN lab robots.  This system continues to be used largely unchanged for sample tracking in the CASM laboratory.  During this time I also contributed to very early versions of the COSMIC database.</p> <p>During 2004 CGP moved heavily into capillary sequencing (also known as Sanger sequencing). Once the LIMS was adapted I moved on to developing a high throughput pipeline to run the autoCSA analysis software.  The control software was written using JAVA, and a public desktop application, a.k.a. StandaloneCSA, was created for external use and released early 2007.  This was a large project involving many developers working together on various aspects including database design, the analysis pipeline itself and a web-application used for visual inspection of the called variants by scientific staff.</p>"},{"location":"profile/#2007-2009","title":"2007-2009","text":"<ul> <li>Clinical Programmer</li> </ul> <p>In 2007 I left the CGP and joined Roche Pharmaceuticals as a Clinical Programmer.  Here I gained experience of good clinical practice while building data capture and cleaning tools using Oracle Clinical.</p>"},{"location":"profile/#2009-2022","title":"2009-2022","text":"<ul> <li>Seniour Bioinformatician</li> <li>Principal Software Developer / Bioinformatician</li> </ul> <p>After 2 years at Roche I returned to the CGP in 2009.  Although this may seem an unusual career choice the advent of next generation sequencing had significantly changed the challenges and work being undertaken.  One of the primary differences in the data was the sheer volume being generated.</p>"},{"location":"profile/#analysis-pipelines","title":"Analysis pipelines","text":"<p>The initial project was to create a pipeline to run the mapping of Illumina paired-end sequencing, supporting multiple species and builds.</p> <p>Over 6 months I extended an existing database, developed a new web-application and file tracking system to control the scheduling and flow of data into and out of a Platform LSF compute farm. After the initial 6 months the success of the project resulted in it being extended to support the downstream analysis tools with more of the team moving in to aid in both support and development.  During this phase of the project the web-interfaces were extended significantly to allow the scientific staff to manage their data and analysis.</p> <p>The system proved to be a work-horse for the group for 6 years.  The core jobs submission mechanism has been reworked by the group in the last few years, but the core database and much of the web-application has remained largely unchanged.</p>"},{"location":"profile/#genome-browsers","title":"Genome browsers","text":"<p>As part of the analysis pipeline work I brought GBrowse into the the group as a centrally managed genome browser.  GBrowse relies on server side compute and a traditional database backend to provide the images. In 2012 JBrowse matured to a state where BAM data could be directly handled in the browser.  Due to this, and no further development on GBrowse, we have been actively working with the GMOD community to ensure features of GBrowse that our scientific staff consider important are replicated.  We formally retired our GBrowse instance a few years ago.</p> <p>During 2016 I developed a plugin for JBrowse, proportionalMultiBw (prototyped in my own time) to aid in the visualization of allele fraction in regions of very high sequencing depth.  I've additionally produced a general purpose tool for automating generation of screen shots, see cgpJBrowseToolkit. The new JBrowse 2 embraces this screenshot feature making this a core component of this new version of the tool (not based on my code).</p>"},{"location":"profile/#global-projects","title":"Global projects","text":"<p>In 2014 the CGP became involved in the ICGC PanCancer Analysis of Whole Genomes project. As part of this a significant portion of the IT team spent 6 months preparing all of the core algorithms we use in house for public use.  In addition to this work I was a member of the technical working group advising on tools, mapping strategy and creating tools to aid the submission of the raw sequencing data from the sequencing sites around the world.  Working closely with the IT group at OICR I implemented the 'Sanger' pipeline into the framework to be used in multiple data centers around the world.  This pipeline ran successfully in 13 locations on a variety of base infrastructures including AWS, Azure, OpenStack and traditional HPC.</p> <p>Following on from this project I continue to advise in the ICGC-ARGO data management group and the PPCG technical working group.</p>"},{"location":"profile/#containers-for-reproducible-science","title":"Containers for reproducible science","text":"<p>In 2016 I developed cgpbox as an initial prototype providing our wholegenome analysis platform in a convenient to use docker image.  This was well received and we continue to build on this with the dockstore framework (see above).  Variations on \"cgpbox\" have been used to drive the PPCG project analysis and are in use by ICGC-ARGO at GDC</p> <p>Following on from this I have lead the initiative to convert all of our analysis pipelines to use containers.  This has allowed us to offer our users the ability to fix algorithm versions for the life of their analysis projects.</p>"},{"location":"profile/#full-stack-development-experience","title":"Full stack development experience","text":"<p>During 2017 I prototyped a full-stack system for receipt of external sequencing data via web-interface and S3, backed by web-services hosted in our OpenStack flexible compute farm.  Some of this can be viewed here. Following on from this a small team was formed to take this project further, developing the service architecture to validate the input data.  Sadly the decision was taken to pause this project, however a great deal of new technology was investigated and this has proven to be valuable for subsequent projects.</p>"},{"location":"profile/#readers-note","title":"Readers note","text":"<p>Above I have detailed projects that I have been heavily involved in.  Although I may have worked exclusively on some of these it should be noted that I value the advice and expertise of those around me.  The 'cancerit' team is an excellent group of people from varied backgrounds who draw on each others strengths.  The team is also supported by the core informatics and infrastructure groups at the Sanger Institute.</p>"},{"location":"profile/#2022-onwards","title":"2022 onwards","text":"<ul> <li>Head of Health Informatics Platforms, Health Innovation East<ul> <li>Formerly: Head of Service, Clinical Computational Research Informatics</li> </ul> </li> </ul> <p>Eastern AHSN (Academic Health Science Network) changed its name to Health Innovation East in October 2023.</p> <p>Initially focused on the delivery and development of the CYNAPSE platform to modernise and future-proof the computational infrastructure for genomics and wider \u2018omics\u2019 data across the NIHR Cambridge BioResource Centre (BRC).</p> <p>A significant portion of the position being around requirements gathering and development of a sustainable service delivery team supporting a diverse 'omic research community.</p> <p>2023, role expanded to cover a larger portfolio of work including the NHS Sub-National Secure Data Environment for the  East of England.  The team has now expanded to a total of 6 technical members covering cloud infrastructure, data management and information analysts, supporting projects across Cambridge University Health Partners (CUHP), University of Cambridge, NHS data strategy projects as well as smaller ad-hoc contracts in the research space.</p>"},{"location":"profile/#consultancy-work","title":"Consultancy work","text":"<p>Short term consultancy projects are listed here.</p>"},{"location":"profile/#september-2021-march-2022","title":"September 2021 - March 2022","text":"<p>Eastern AHSN (Academic Health Science Network), project scoping exercise for CYNAPSE.  The aim of CYNAPSE is to modernise and future-proof the computational infrastructure for genomics and wider \u2018omics\u2019 data across the NIHR Cambridge BioResource Centre (BRC).  Insights sought on:</p> <ul> <li>data storage practices for genomic data</li> <li>workflow technologies</li> <li>cloud platforms</li> <li>division of responsibilities</li> </ul>"},{"location":"blogs/","title":"Listing","text":""},{"location":"blogs/#2022","title":"2022","text":"<ul> <li>Markdown TOC and Mkdocs</li> </ul>"},{"location":"blogs/#2021","title":"2021","text":"<ul> <li>Exposing singularity images as executables</li> <li>ssh, proxies and multiplexing</li> <li>Refreshing my site</li> </ul>"},{"location":"blogs/2021/05/updating-my-site/","title":"Refreshing my site","text":"<p>I've decided to refresh this for a couple of reasons:</p> <ol> <li>It looks a little tired.  Originally created doing it \"my way\", mainly as a \"live CV\".</li> <li>I feel like I have something to contribute via blog posts.</li> <li>Interested in including some user interaction/feedback.</li> </ol>"},{"location":"blogs/2021/05/updating-my-site/#choosing-a-site-generator","title":"Choosing a site generator","text":"<p>My original site already worked from markdown files and I want to continue like this.  I pretty much write all my documentation in markdown.  Here are the items I looked at:</p> <ul> <li>Jekyll</li> <li>MkDocs</li> <li>Material for MkDocs</li> </ul>"},{"location":"blogs/2021/05/updating-my-site/#jekyll","title":"Jekyll","text":"<p>Jekyll is a well established static site generator, with direct integration into github pages.  This seemed like the obvious place to start as I'm going to host it on github-pages.</p> <p>I quickly found that if you've worked with other site generators Jekyll can feel overly complex.  I suspect this is due to flexibility, but for what I needed it was too much.  I also wasn't a big fan of the available themes.</p>"},{"location":"blogs/2021/05/updating-my-site/#mkdocs","title":"MkDocs","text":"<p>Full disclosure, I've used MkDocs before for my work at Sanger and I already know how to use it.</p> <p>Quite simply if you want to provide a site that is easy to navigate all you need to do in MkDocs is structure things in a document tree as you want to see it in the site menu:</p> <pre><code>docs/\n\u251c\u2500\u2500 blogs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2021\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 05\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 updating-my-site.md\n\u2514\u2500\u2500 index.md\n</code></pre> <p></p> <p>The above requires no configuration at all, but obviously you still want to do some.</p> <p>Before I go further, I want my site to have a clean look and feel.  MkDocs has a limited number of themes, Read the Docs isn't bad but there is an extension project which I recently used and have found it good.  As I'm leaning towards MkDocs over Jekyll, lets take that next step before I get bogged down on layout and content...</p>"},{"location":"blogs/2021/05/updating-my-site/#mkdocs-material","title":"MkDocs Material","text":"<p>This was the selected tool (at time of writing).</p> <p>As the title suggests, this is MkDocs focused on Material Design components.</p> <p>The first thing you notice when you apply the material theme is that you automatically get a left and right sidebar.  The left sidebar gives you the site navigation, very similar to the document tree:</p> <p></p> <p>This will automatically change to a hamburger menu button if the view is narrow.</p> <p>On the right the table of content for the current document is shown.</p>"},{"location":"blogs/2021/05/updating-my-site/#interaction","title":"Interaction","text":"<p>As I'm thinking of writing blog posts semi-regularly it makes sense to give people the ability to interact via comments.</p> <p>This is hosted in the GitHub eco-system and I don't really want to be pushing people to another service for comments so I did a little digging and found utterances.</p> <p>This basically uses the repositories issue tracking system to allow comments, see it in action below.</p>"},{"location":"blogs/2021/05/updating-my-site/#comments","title":"Comments","text":""},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/","title":"ssh, proxies and multiplexing","text":"<p>It can be time consuming to get your ssh connections with proxies and multiplexing working well.  Here I've dumped what I used from a Mac laptop to connect to my work system.  Hosts and routes have been changed.</p> <p>It's mainly for my reference, but may be useful for others.</p>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#options","title":"Options","text":"<p>A longer example can be found at the end of this document, not all options will be detailed.</p> <p>As always see the man page for full explanations or other uses/defaults.</p>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#proxyjump","title":"ProxyJump","text":"<p>If you need to connect to a host as more than one user, which doesn't match the ssh gateway user, you need to include the user that authenticates at the ssh-gateway in the <code>ProxyJump</code> directive.</p> <p>e.g.</p> <ul> <li>ssh-gateway user = bob</li> <li>internal-network users = bob,fred</li> </ul> <pre><code># if\n# Host work\n#   ProxyJump bob@ssh.someplace.ac.uk\nbob@home$ ssh work\n... ssh auth ...\nbob@work$\nbob@home$ ssh fred@work\n# uses bob to pass through ssh-gateway\n</code></pre>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#serveraliveinterval","title":"ServerAliveInterval","text":"<p>How frequently to check the server is still accessible:</p> <ul> <li>useful to ensure connections aren't dropped</li> <li>prevent connection being dropped due to inactivity</li> </ul>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#serveralivecountmax","title":"ServerAliveCountMax","text":"<p>How many <code>ServerAliveInterval</code> messages without a response are acceptable before abandoning the connection.</p>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#controlmaster","title":"ControlMaster","text":"<p>Send data for the same user and same host down a single TCP connection (also see ControlPath).  Saves authenticating each time you open another terminal for the same host+user... type less :)</p>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#full-example","title":"Full example","text":"<p>Includes comments that may point you to useful docs:</p> <pre><code>Host *\n  XAuthLocation /opt/X11/bin/xauth\n  Compression yes\n  ServerAliveInterval 60\n  ServerAliveCountMax 2\n  ForwardX11Timeout 100w\n  Ciphers aes128-ctr\n  # share 1 ssh connection for same host/port/user\n  ControlMaster auto\n  ControlPath ~/.ssh/ssh_mux_%h_%p_%r\n\nHost proxies\n  HostName somehost\n  # do ssh gateway dance and drop me onto the host I really want\n  ProxyJump ssh.someplace.ac.uk\n  # Point your SOCKS proxy to localhost:8999 to pass all network through your\n  # ssh connection\n  #  - recommend setting bypass rules for zoom/webex etc\n  DynamicForward 8999\n  # Forwarding - good for databases\n  LocalForward 25000 172.XX.XX.XXX:1521\n\n# a machine on my home network with no DNS name:\nHost my-nano\n  HostName 192.168.0.90\n\n# a host within company network, without DNS name:\nHost unnamed-host\n  HostName 172.27.22.23\n  ProxyJump ssh.someplace.ac.uk\n\n# a host within company network\nHost named-host\n  HostName somehost\n  ProxyJump user@ssh.someplace.ac.uk\n</code></pre>"},{"location":"blogs/2021/09/ssh-proxies-and-multiplexing/#comments","title":"Comments","text":""},{"location":"blogs/2021/10/containers-as-executables/","title":"Exposing singularity images as executables","text":"<p>It can be difficult to expose some tools where they have extensive dependencies without polluting your environment. Environment modules can be used to handle this, but the interdependencies can be complex.</p> <p>In pipelines you need to ensure that versions of tools/algorithms are compatible and known to have been tested together so it is common to use containers to ensure the tested \"unit\" is the one that is in use.</p> <p>Unfortunately many environments don't allow you to use docker, but many will allow the use of singularity containers as they don't have the root escalation issues of plain docker.</p>"},{"location":"blogs/2021/10/containers-as-executables/#get-a-singularity-image","title":"Get a singularity image","text":"<p>Most tools with have a versioned docker image, singularity can pull this and convert it to a container that can be safely used without fear of root escalation:</p> <pre><code>$ singularity pull docker://quay.io/wtsicgp/pcap-core:5.7.0\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\n...\nINFO:    Creating SIF file...\n$ ls -lh *.sif\n-rwxrwx--- 1 kr2 ubuntu 157M Oct 18 15:20 pcap-core_5.7.0.sif\n</code></pre> <p>I'll leave you to decide the best place for your images, in this example images are copied into:</p> <pre><code>    /software/singularity-images/${program}/${program}_${version}.sif\ne.g.\n    /software/singularity-images/pcap-core/pcap-core_5.7.0.sif\n</code></pre>"},{"location":"blogs/2021/10/containers-as-executables/#make-the-singularity-image-available-via-environment-modules","title":"Make the singularity image available via environment modules","text":"<p>We need a convenient way to set up environment variables which can point to this image and simplify use for users. Modulefiles can provide this with a little lateral thinking and a few \"shim\" scripts.</p> <p>This blog will not cover the intricacies of Environment Modules, see the official docs.</p> <p>Here we setup a basic module file to make some variables available, keeping the elements you need to change at the very start of the file.</p> <p>File: <code>/software/modules/modulefiles/pcap-core/5.7.0</code></p> <pre><code>#%Module\nset version 5.7.0\nset program pcap-core\nset imagepath /software/singularity-images/$program/${program}_${version}.sif\nset moduleinfo \"Exposes $program tools via singularity, lustre, nfs and home accessible\"\nset home $env(HOME)\nset sing_base \"--cleanenv --home ${home}:${home} --bind ${home}:${home} ${imagepath}\"\nmodule-whatis \"$moduleinfo\"\nmodule load singularity\nset-alias \"${program}-shell\" \"singularity shell $sing_base\"\nset-alias \"${program}-exec\" \"singularity exec $sing_base\"\nprepend-path PATH /software/modules/installs/$program/shim-bin\nsetenv SING_PREFIX_PCAPCORE \"singularity exec $sing_base\"\n</code></pre> <p>The main items of note in the above script are:</p> line description intent 2 Version of container Single place to modify 3 Name of container Single place to modify 7 Bulk of singularity options Ease of reading later commands 9 Load default singularity module assumes you have this available 10 Singularity shell alias for this container Convenience for interactive/debug use 11 Singularity exec alias for this container Convenience for interactive/debug use 12 Add \"virtual\" script location to path See \"shim scripts\", allows tab complete/discovery 13 Container specific variable See \"shim scripts\" <p>Assuming <code>/software/modules/modulefiles</code> is in <code>$MODULEPATH</code>:</p> <pre><code>$ which bwa_mem.pl\n# not found\n$ module load pcap-core/5.7.0\n$ pcap-core-shell\nSingularity&gt; which bwa_mem.pl\n/opt/wtsi-cgp/bin/bwa_mem.pl\nSingularity&gt; exit\n$ which bwa_mem.pl\n# not found\n</code></pre> <p>This is a good start, but it's not the same as just calling a script.</p> <p>Lets talk about shim-scripts.</p>"},{"location":"blogs/2021/10/containers-as-executables/#shim-scripts","title":"shim scripts","text":"<p>A shim script is a very minimal script that can hide the complexity of calling the singularity command and behave as though the real script is available.</p> <p>Here we'll create a shim-script for the <code>bwa_mem.pl</code> script within the pcap-core container:</p> <p>File: <code>/software/modules/installs/pcap-core/shim-bin/bwa_mem.pl</code></p> <pre><code>#!/usr/bin/env bash\nexec $SING_PREFIX_PCAPCORE $(basename $0) \"$@\"\n</code></pre> <p>Note that the <code>$SING_PREFIX_PCAPCORE</code> variable from the modulefile is referenced.</p> <p>Now you can simply call (or tab-complete) <code>bwa_mem.pl</code> from the command line provided you have loaded the module:</p> <pre><code>$ module load pcap-core/5.7.0\n$ bwa_mem.pl -h\nUsage:\n    bwa_mem.pl [options] [file(s)...]\n...\n</code></pre> <p>For each command you want to expose to the user create a shim-script.</p> <p>Note: you shouldn't expose all the tools in the container, just the unique items the container was built for.</p>"},{"location":"blogs/2021/10/containers-as-executables/#comments","title":"Comments","text":""},{"location":"blogs/2022/04/mkdocs-toc-indent/","title":"Markdown TOC and Mkdocs","text":"<p>There is a small issue with the tables-of-contents generated to confirm with MD007 in that the lint/standard is 2 spaces, while Mkdocs required 4 spaces to work.</p> <p>Thankfully a kind soul has developed a plugin to stop this being a pain when you want your TOC to automatically update on save, but not get corrupted.</p> <p><code>mdx_truly_sane_lists</code> is what you need.  Simply install it as part of your build environment and append it to the <code>mkdocs.yaml</code> markdown_extensions section:</p> <pre><code>markdown_extensions:\n  - ...\n  - mdx_truly_sane_lists\n</code></pre>"},{"location":"blogs/2022/04/mkdocs-toc-indent/#comments","title":"Comments","text":""}]}
